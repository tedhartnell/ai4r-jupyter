{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af0d909",
   "metadata": {},
   "source": [
    "# Dynamic Value Iteration Planning\n",
    "By: Edwin Hartnell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33ca09",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Based upon the *Better Markov Decision Process (MDP) Tools* ([bettermdptools](https://github.com/jlm429/bettermdptools)) by John Mansfield et al. And based upon the [Gym API](https://www.gymlibrary.dev/) for reinforcement learning. The example below was adapted from the [Frozen Lake](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/) example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ae7fe",
   "metadata": {},
   "source": [
    "## The Why\n",
    "\n",
    "Self-driving vehicles know where they are located, and they know the location of their goal. They can use planning algorithms like Value Iteration to calculate a set of driving directions based upon an optimal policy. But the vehicle can also learn on the way. It can encounter unknown obstacles and be forced to adjust its policy.\n",
    "\n",
    "This Jupyter Workbook provides example code that can be adapted into a real-world experiment for a robotic kit car."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc319e2",
   "metadata": {},
   "source": [
    "## Grid World\n",
    "\n",
    "In this experiment, an 8x8 grid world is provided for the robotic kit car. There is a starting location (labeled 'S') and a goal location (labeled 'G'). While there may be many holes in the road along the way, the robot does not yet know about these. It only knows the starting location and the goal, and it calculates an initial optimal path based upon what it knows. But as the robot encounters each hole, it must re-evaluate its optimal policy and come up with a new plan.\n",
    "\n",
    "While this code is self-contained and can run without robotic hardware, it could be adapted for a real-world experiment by taping out a grid on the floor. Black tape can mark out holes in the road, and the robotic car can identify these holes and avoid them using its Line Detector hardware.\n",
    "\n",
    "![Grid World Planning](images/grid-world-planning-002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417ea58d",
   "metadata": {},
   "source": [
    "## Required Hardware\n",
    "\n",
    "In addition to the Line Detector underneath the robot, this experiment also suits robotic kit cars with **mecanum wheels**. These allow the car to move left, right, forwards, and backwards without turning. This simulates the north, south, east, and west directions followed on the map.\n",
    "\n",
    "![Robotic Kit Car with Mecanum Wheels](images/mecanum-wheels-003.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11de433",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0c9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import imageio.v3 as iio\n",
    "from operator import add\n",
    "\n",
    "# Using the local forked version of bettermdptools\n",
    "# This version provides extra output from the Planner that is easy to plot\n",
    "# But the public version of bettermdptools should also work with some modification\n",
    "import gym\n",
    "from library.gt09_bettermdptools.algorithms.planner import Planner\n",
    "from library.gt09_bettermdptools.examples.plots import Plots\n",
    "\n",
    "# CSV outputs are going to the output director\n",
    "# PNG plots are going to the plots director\n",
    "# GIF is created from the plots and saved in the movies directory\n",
    "if not os.path.exists(\"./output\"): os.makedirs(\"./output\")\n",
    "if not os.path.exists(\"./plots\"): os.makedirs(\"./plots\")\n",
    "if not os.path.exists(\"./movies\"): os.makedirs(\"./movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2df95",
   "metadata": {},
   "source": [
    "## Grid World\n",
    "\n",
    "Two grid world maps are used in this simulation. The first is the *actual* grid world which maps the location of all the holes. The second is the *discovered* grid world that is built up by the robot over time. At first the robot only knows about the starting location 'S' and the goal location 'G'. But it discovers holes 'H' along the way that it must drive around.\n",
    "\n",
    "Note that the entire grid world is ringed by holes. This is to keep the vehicle away from the boundaries when the roads are slippery and non-determinism is introduced into the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d019b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Grid World 8x8 #####\n",
    "#\n",
    "# H = Hole not accessible by the robot\n",
    "# S = Start\n",
    "# G = Goal\n",
    "# F = Free space where the robot can roam\n",
    "# Actually F = Frozen from the original Frozen Lake example\n",
    "#\n",
    "# The actual grid world is ringed by holes that the robot cannot enter\n",
    "grid_world_8x8_actual = [\n",
    "    \"HHHHHHHH\",\n",
    "    \"HSFFFFFH\",\n",
    "    \"HFHHHHFH\",\n",
    "    \"HFHFFFFH\",\n",
    "    \"HFHFHHHH\",\n",
    "    \"HFHFFFFH\",\n",
    "    \"HFHFHHGH\",\n",
    "    \"HHHHHHHH\"\n",
    "]\n",
    "\n",
    "# Originally the robot only knows about the boundaries, the start and the goal\n",
    "# When the robot discovers a hole it will re-calculate the optiaml path to the goal\n",
    "grid_world_8x8_discovered = [\n",
    "    \"HHHHHHHH\",\n",
    "    \"HSFFFFFH\",\n",
    "    \"HFFFFFFH\",\n",
    "    \"HFFFFFFH\",\n",
    "    \"HFFFFFFH\",\n",
    "    \"HFFFFFFH\",\n",
    "    \"HFFFFFGH\",\n",
    "    \"HHHHHHHH\"\n",
    "]\n",
    "start_location = [1, 1]\n",
    "goal_location = [6, 4]\n",
    "# Determine the start location if the Grid World were to be flattend to 1-dimension\n",
    "flat_start_location = start_location[0] * len(grid_world_8x8_actual) + start_location[1]\n",
    "\n",
    "# The robot can only move left, down, right, and up (not diagonally)\n",
    "# A real-world experiment would suite robots with mecanum wheels\n",
    "robot_actions = [\n",
    "    [ 0, -1], # Left\n",
    "    [ 1,  0], # Down\n",
    "    [ 0,  1], # Right\n",
    "    [-1,  0]  # Up\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26856a1",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "The grid world can be modeled as a Markov Decision Process (MDP). And the optimal policy can be calculated using Value Iteration. In this *Run_Value_Iteration* routine, Value Iteration is used to calculate the optimal policy, which is then converted into a grid corresponding to the grid world. The policy is also saved as two heatmaps png-files which map both the optimal actions from each location, along with their corresponding location values. These png-files are later converted into an animated gif movie showing how the robotic car updates the policy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c48823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Value Iteration on the known Grid World map and return the optimal policy\n",
    "#\n",
    "#   Parameters:\n",
    "#       grid_world = The grid world created by gym.make()\n",
    "#       grid_values_heat_map_values_filenamesquare_grid_size = The dimensions of one side of the grid world - for example, '8' indicates an 8x8 grid world\n",
    "#       flat_start_location = The element number of the starting location if the grid world were flattened into 1-dimension\n",
    "#       step_id = The step identifier to add to the saved plots and results\n",
    "#\n",
    "#   Return:\n",
    "#       value_iteration_pi_grid = The actions to take at every location in the grid world\n",
    "#       grid_values_heat_map_actions_filename = The filename of the heatmap actions created during this step for inclusion in the GIF animation\n",
    "#       grid_values_heat_map_values_filename = The filename of the heatmap values created during this step for inclusion in the GIF animation\n",
    "#\n",
    "def Run_Value_Iteration(grid_world, square_grid_size, grid_label, flat_start_location, step_id):\n",
    "    value_iteration_V, value_iteration_V_track, value_iteration_pi, value_iteration_pi_track = Planner(grid_world.env.P).value_iteration( gamma=1.0, n_iters=1000, theta=1e-4 )\n",
    "    grid_values_heat_map_actions_filename = f'plots/GridWorld{grid_label}_ValueIteration_HeatmapActions_{step_id}.png'\n",
    "    Plots.grid_values_heat_map_actions(env=grid_world.env, data=value_iteration_V, policy_actions=value_iteration_pi_track[-1], label=f'{grid_label} Value Iteration - State Actions', size=square_grid_size, filename=grid_values_heat_map_actions_filename)\n",
    "    grid_values_heat_map_values_filename = f'plots/GridWorld{grid_label}_ValueIteration_HeatmapValues_{step_id}.png'\n",
    "    Plots.grid_values_heat_map_values(env=grid_world.env, data=value_iteration_V, policy_actions=value_iteration_pi_track[-1], label=f'{grid_label} Value Iteration - State Values', size=square_grid_size, filename=grid_values_heat_map_values_filename)\n",
    "    print(f'value_iteration_V[start].max = {value_iteration_V[flat_start_location].max()}')\n",
    "    output_dictionary = {f'GridWorld{grid_label}_value_iteration_V_{step_id}':value_iteration_V, f'GridWorld{grid_label}_value_iteration_V_track_{step_id}':value_iteration_V_track[0:value_iteration_pi_track.shape[0]], f'GridWorld{grid_label}_value_iteration_pi_track_{step_id}':value_iteration_pi_track, f'GridWorld{grid_label}_value_iteration_pi_{step_id}':value_iteration_pi_track[-1]}\n",
    "    Save_Results_Output(output_dictionary=output_dictionary, output_directory='output/')\n",
    "\n",
    "    # Return the Optimal Policy as a grid\n",
    "    value_iteration_pi_grid = value_iteration_pi_track[-1]\n",
    "    value_iteration_pi_grid = np.array(value_iteration_pi_grid)\n",
    "    value_iteration_pi_grid = value_iteration_pi_grid.reshape((square_grid_size, -1))\n",
    "    value_iteration_pi_grid = value_iteration_pi_grid.tolist()\n",
    "    return value_iteration_pi_grid, grid_values_heat_map_actions_filename, grid_values_heat_map_values_filename\n",
    "\n",
    "# Check if the current location of the robot is in-bounds\n",
    "# The outer rim of holes around the grid world is considered out-of-bounds\n",
    "def Check_Location_In_Bounds(location):\n",
    "    if location[0] > 0 and location[0] < len(grid_world_8x8_actual) - 1 \\\n",
    "        and location[1] > 0 and location[1] < len(grid_world_8x8_actual[0]) - 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Save the returned results as CSV files\n",
    "def Save_Results_Output(output_dictionary, output_directory='output/'):\n",
    "    for output_name, output_results in output_dictionary.items():\n",
    "        file_path_name = output_directory + output_name + '.csv'\n",
    "        if 'pi_track' in file_path_name or '_pi_' in file_path_name or '_C.csv' in file_path_name:\n",
    "            np.savetxt(file_path_name, output_results, fmt=\"%d\", delimiter=',') # Save as an integer\n",
    "        else:\n",
    "            np.savetxt(file_path_name, output_results, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6664919",
   "metadata": {},
   "source": [
    "## Run the Grid World Planning\n",
    "\n",
    "The robotic car initially follows the first optimal plan generated before any holes have been discovered. When a hole is discovered, it is added to the map and the optimal plan is recalculated. This repeats until the robotic car makes it all the way to the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189a2a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting GridWorld8x8\n",
      "12345678901=[11]\n",
      "runtime = 0.01 seconds\n",
      "value_iteration_V[start].max = 1.0\n",
      "WARNING: Whoops - cannot go that way to: [6, 2]. Calculating new policy.\n",
      "12345678901=[11]\n",
      "runtime = 0.01 seconds\n",
      "value_iteration_V[start].max = 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate the new location of the robot based upon its current location and the the optimal policy\u001b[39;00m\n\u001b[1;32m     23\u001b[0m action \u001b[38;5;241m=\u001b[39m value_iteration_pi_grid[current_location[\u001b[38;5;241m0\u001b[39m]][current_location[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m---> 24\u001b[0m new_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobot_actions\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Check if the robot is still in bounds\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Check_Location_In_Bounds(new_location):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "##### Start Running the Grid World Planning #####\n",
    "#################################################\n",
    "\n",
    "# The Grid World is based upon the Frozen Lake Markov Decision Process (MDP)\n",
    "step_id = 0\n",
    "grid_label = f'{len(grid_world_8x8_discovered)}x{len(grid_world_8x8_discovered)}'\n",
    "grid_world = gym.make('FrozenLake-v1', desc=grid_world_8x8_discovered, map_name=f'GridWorld{grid_label}_{step_id}', is_slippery=False, render_mode=None)\n",
    "print(f'\\nStarting GridWorld{grid_label}')\n",
    "\n",
    "# Generate an initial policy knowing only the start location and the goal\n",
    "value_iteration_pi_grid, grid_values_heat_map_actions_filename, grid_values_heat_map_values_filename = Run_Value_Iteration(grid_world, len(grid_world_8x8_discovered), grid_label, flat_start_location, step_id)\n",
    "value_iteration_pi_grid_previous = value_iteration_pi_grid.copy()\n",
    "grid_values_heat_map_actions_files = [grid_values_heat_map_actions_filename]\n",
    "grid_values_heat_map_values_files = [grid_values_heat_map_values_filename]\n",
    "\n",
    "# The Robot follows the latest optimal policy path from the start location to the goal\n",
    "current_location = start_location\n",
    "while current_location != goal_location:\n",
    "    step_id += 1\n",
    "\n",
    "    # Calculate the new location of the robot based upon its current location and the the optimal policy\n",
    "    action = value_iteration_pi_grid[current_location[0]][current_location[1]]\n",
    "    new_location = list( map(add, current_location, robot_actions[action]) )\n",
    "\n",
    "    # Check if the robot is still in bounds\n",
    "    if not Check_Location_In_Bounds(new_location):\n",
    "        print(f'ERROR: The optimal policy will take the robot out-of-bounds to: {new_location}')\n",
    "        break\n",
    "\n",
    "    # Check if the robot has reached the goal\n",
    "    if grid_world_8x8_actual[new_location[0]][new_location[1]] == 'G':\n",
    "        print(f'SUCCESS: The robot has reached the goal at: {new_location}')\n",
    "        break\n",
    "\n",
    "    # Check if the robot can safely move to the new location\n",
    "    if grid_world_8x8_actual[new_location[0]][new_location[1]] != 'H':\n",
    "        current_location = new_location\n",
    "        continue\n",
    "\n",
    "    # Otherwise the robot would hit a hole if it were to follow the suggested action\n",
    "    print(f'WARNING: Whoops - cannot go that way to: {new_location}. Calculating new policy.')\n",
    "\n",
    "    # Add the hole to the discovered map and recalculate the optimal policy\n",
    "    grid_world_8x8_discovered[new_location[0]] = grid_world_8x8_discovered[new_location[0]][:new_location[1]] + 'H' + grid_world_8x8_discovered[new_location[0]][new_location[1]+1:]\n",
    "    grid_world = gym.make('FrozenLake-v1', desc=grid_world_8x8_discovered, map_name=f'GridWorld{grid_label}_{step_id}', is_slippery=False, render_mode=None)\n",
    "    value_iteration_pi_grid, grid_values_heat_map_actions_filename, grid_values_heat_map_values_filename = Run_Value_Iteration(grid_world, len(grid_world_8x8_discovered), grid_label, flat_start_location, step_id)\n",
    "    grid_values_heat_map_actions_files.append(grid_values_heat_map_actions_filename)\n",
    "    grid_values_heat_map_values_files.append(grid_values_heat_map_values_filename)\n",
    "\n",
    "    # Check if the optiaml policy has changed\n",
    "    if value_iteration_pi_grid == value_iteration_pi_grid_previous:\n",
    "        print(f'ERROR: The optimal policy has not changed even though a new hole was discovered at: {new_location}')\n",
    "        break\n",
    "    else:\n",
    "        value_iteration_pi_grid_previous = value_iteration_pi_grid.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e96684",
   "metadata": {},
   "source": [
    "## Animated GIF Movies\n",
    "\n",
    "Once the robotic car makes it to the goal, all of its intermediate policies are combined into animated gif movies. These show how the vehicle adapts to the changes it discovers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2070823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The robot has finished running through the grid world and has developed the best optimal plan it can\n",
    "# Now create the policy GIF animations based upon what the robot has learned\n",
    "\n",
    "# Create Gif Animation - Heatmap Actions\n",
    "plots = []\n",
    "for filename in grid_values_heat_map_actions_files:\n",
    "    plots.append(iio.imread(filename))\n",
    "iio.imwrite(f'movies/GridWorld{grid_label}_ValueIteration_HeatmapActions.gif', plots, loop=0, duration=1000) # duration(in ms): `fps=50` == `duration=20` (1000 * 1/50)\n",
    "\n",
    "# Create Gif Animation - Heatmap Values\n",
    "plots = []\n",
    "for filename in grid_values_heat_map_values_files:\n",
    "    plots.append(iio.imread(filename))\n",
    "iio.imwrite(f'movies/GridWorld{grid_label}_ValueIteration_HeatmapValues.gif', plots, loop=0, duration=1000) # duration(in ms): `fps=50` == `duration=20` (1000 * 1/50)\n",
    "\n",
    "print(f'Finished GridWorld{grid_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0bf37e",
   "metadata": {},
   "source": [
    "## Actions Heatmap\n",
    "\n",
    "Shows the evolving actions for the optimal policy updated each time the robot encounters a hole.\n",
    "\n",
    "![Value Iteration Heatmap Actions](movies/GridWorld8x8_ValueIteration_HeatmapActions.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0233f9d",
   "metadata": {},
   "source": [
    "## Values Heatmap\n",
    "\n",
    "Shows the evolving values for each location calculated by Value Iteration updated each time the robot encounters a hole.\n",
    "\n",
    "![Value Iteration Heatmap Values](movies/GridWorld8x8_ValueIteration_HeatmapValues.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
